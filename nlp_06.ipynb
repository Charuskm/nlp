{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roll No:225229106"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Document Similarity using Doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in e:\\anaconda\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in e:\\anaconda\\lib\\site-packages (from gensim) (5.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in e:\\anaconda\\lib\\site-packages (from gensim) (1.21.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in e:\\anaconda\\lib\\site-packages (from gensim) (1.7.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in e:\\anaconda\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in e:\\anaconda\\lib\\site-packages (from gensim) (5.1.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in e:\\anaconda\\lib\\site-packages (from gensim) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.17.0 in e:\\anaconda\\lib\\site-packages (from gensim) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[\"I love machine learning. Its awesome.\",\n",
    "\"I love coding in python\",\n",
    "\"I love building chatbots\",\n",
    "\"they chat amagingly well\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Create TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data=[TaggedDocument(words=word_tokenize(d.lower()),tags=[str(i)]) for i,d in enumerate(data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "#model parameters\n",
    "vec_size=20\n",
    "alpha=0.025\n",
    "\n",
    "#create model\n",
    "model=Doc2Vec(vector_size=vec_size,\n",
    "             alpha=alpha,\n",
    "             min_alpha=0.00025,\n",
    "             min_count=1,\n",
    "             dm=1)\n",
    "\n",
    "#build vocabulary\n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "#shuffle data\n",
    "tagged_data=utils.shuffle(tagged_data)\n",
    "\n",
    "#train Doc2Vec model\n",
    "model.train(tagged_data,\n",
    "        total_examples=model.corpus_count,\n",
    "        epochs=30)\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Find Similar documents for the given document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1_infer [-0.0233888   0.01124523 -0.02402804 -0.0036835  -0.01628511  0.00649093\n",
      " -0.02432564 -0.01641418 -0.01301352 -0.01157944  0.02084662 -0.00026435\n",
      "  0.00470413  0.01869185 -0.00888484 -0.00149818 -0.01963419  0.02390047\n",
      "  0.0207266   0.01899391]\n",
      "[('2', 0.3225879669189453), ('0', 0.2904272973537445), ('3', 0.22423316538333893)]\n",
      "[-0.01951095  0.01334204 -0.02924857  0.01347412  0.03016925 -0.04165563\n",
      " -0.04264015 -0.05076316  0.02415931 -0.04680936  0.0294119   0.0347948\n",
      " -0.03395152 -0.02405545 -0.00715238  0.00799955 -0.00750972 -0.04314583\n",
      " -0.01919778  0.0087441 ]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "model=Doc2Vec.load(\"d2v.model\")\n",
    "\n",
    "#to find the vector of a document which is not in training data\n",
    "\n",
    "test_data=word_tokenize(\"I love chatbots\".lower())\n",
    "v1=model.infer_vector(test_data)\n",
    "print(\"v1_infer\",v1)\n",
    "\n",
    "#to find most similar doc using tags\n",
    "similar_doc=model.dv.most_similar('1')\n",
    "print(similar_doc)\n",
    "\n",
    "#to find vector of doc in training data using tags or\n",
    "#In other words,printing the vector of document at index 1 in training data\n",
    "\n",
    "print(model.dv[\"1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question-1. Train the following documents using Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docs = [\"the house had a tiny little mouse\",\n",
    "        \"the mouse ran away from the house\",\n",
    "        \"the cat finally ate the mouse\",\n",
    "        \"the end of the mouse story\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_docs=[TaggedDocument(words=word_tokenize(d.lower()),tags=[str(i)]) for i,d  in enumerate(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#model parameters\n",
    "vec_size=20\n",
    "alpha=0.025\n",
    "\n",
    "#create model\n",
    "model=Doc2Vec(vector_size=vec_size,\n",
    "             alpha=alpha,\n",
    "             min_alpha=0.00025,\n",
    "             min_count=1,\n",
    "             dm=1)\n",
    "\n",
    "#build vocabulary\n",
    "model.build_vocab(tagged_docs)\n",
    "\n",
    "#shuffle data\n",
    "tagged_docs=utils.shuffle(tagged_docs)\n",
    "\n",
    "#train Doc2Vec model\n",
    "model.train(tagged_docs,\n",
    "           total_examples=model.corpus_count,\n",
    "           epochs=30)\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question-2.Find the most similar TWO documents for the query document \"cat stayed in the house.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1_infer [ 0.00818743  0.01028682 -0.01371128  0.01616325 -0.01162654 -0.00040655\n",
      "  0.02408683 -0.0137874   0.00615388  0.01211927  0.00603802 -0.00720569\n",
      " -0.00919611 -0.00304825  0.0237018  -0.018138    0.01383917  0.01281842\n",
      "  0.01159879  0.02252774]\n",
      "[('3', 0.3407263457775116), ('1', 0.3237858712673187), ('0', -0.11142323911190033)]\n",
      "[-0.01069595 -0.03664056  0.02066249 -0.04309044  0.01412016 -0.02351031\n",
      "  0.00293231 -0.01072208  0.02677087 -0.04064897 -0.01075689 -0.00030844\n",
      " -0.03429706 -0.03365558 -0.01034019  0.04419803 -0.00640141  0.01814361\n",
      " -0.02967732  0.0445753 ]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "model=Doc2Vec.load(\"d2v.model\")\n",
    "\n",
    "#to find the vector of a document which is not in training data\n",
    "test_data=word_tokenize(\"cat stayed in the house\".lower())\n",
    "v1=model.infer_vector(test_data)\n",
    "print(\"v1_infer\",v1)\n",
    "\n",
    "#to find most similar doc using tags\n",
    "similar_doc=model.dv.most_similar('2')\n",
    "print(similar_doc)\n",
    "\n",
    "#to find vector of doc in training data using tags\n",
    "print(model.dv[\"2\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
