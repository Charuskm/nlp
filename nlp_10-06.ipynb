{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roll:No:225229106"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB-10:Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1=\"Rajkumar said on Monday that WASHINGTON--In the wake of string of abuse by new york police officers in the 1990s,Loretta E.Lynch,the Top federal prosecutor in brooklyn ,spoke forcefully about the pain of a broken trust that Africa-Americans felt and said the responsibility for repairing generations of communication and mistrust fell to law enforcement\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('PERSON', [('Rajkumar', 'NNP')])]\n"
     ]
    }
   ],
   "source": [
    "tokens=word_tokenize(sentence1)\n",
    "tags=pos_tag(tokens)\n",
    "ne_tree=ne_chunk(tags)\n",
    "print(ne_tree[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_tree=ne_chunk(pos_tag(word_tokenize(sentence1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PERSON Rajkumar/NNP)\n",
      "('said', 'VBD')\n",
      "('on', 'IN')\n",
      "('Monday', 'NNP')\n",
      "('that', 'IN')\n",
      "(ORGANIZATION WASHINGTON/NNP)\n",
      "('--', ':')\n",
      "('In', 'IN')\n",
      "('the', 'DT')\n",
      "('wake', 'NN')\n",
      "('of', 'IN')\n",
      "('string', 'NN')\n",
      "('of', 'IN')\n",
      "('abuse', 'NN')\n",
      "('by', 'IN')\n",
      "('new', 'JJ')\n",
      "('york', 'NN')\n",
      "('police', 'NN')\n",
      "('officers', 'NNS')\n",
      "('in', 'IN')\n",
      "('the', 'DT')\n",
      "('1990s', 'CD')\n",
      "(',', ',')\n",
      "(PERSON Loretta/NNP E.Lynch/NNP)\n",
      "(',', ',')\n",
      "('the', 'DT')\n",
      "(ORGANIZATION Top/NNP)\n",
      "('federal', 'JJ')\n",
      "('prosecutor', 'NN')\n",
      "('in', 'IN')\n",
      "('brooklyn', 'NN')\n",
      "(',', ',')\n",
      "('spoke', 'VBD')\n",
      "('forcefully', 'RB')\n",
      "('about', 'IN')\n",
      "('the', 'DT')\n",
      "('pain', 'NN')\n",
      "('of', 'IN')\n",
      "('a', 'DT')\n",
      "('broken', 'JJ')\n",
      "('trust', 'NN')\n",
      "('that', 'IN')\n",
      "('Africa-Americans', 'NNP')\n",
      "('felt', 'VBD')\n",
      "('and', 'CC')\n",
      "('said', 'VBD')\n",
      "('the', 'DT')\n",
      "('responsibility', 'NN')\n",
      "('for', 'IN')\n",
      "('repairing', 'VBG')\n",
      "('generations', 'NNS')\n",
      "('of', 'IN')\n",
      "('communication', 'NN')\n",
      "('and', 'CC')\n",
      "('mistrust', 'NN')\n",
      "('fell', 'VBD')\n",
      "('to', 'TO')\n",
      "('law', 'NN')\n",
      "('enforcement', 'NN')\n"
     ]
    }
   ],
   "source": [
    "for i in ne_tree:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Counter({'Rajkumar': 1, 'NNP': 1})]\n",
      "[Counter({'s': 1, 'a': 1, 'i': 1, 'd': 1}), Counter({'V': 1, 'B': 1, 'D': 1})]\n",
      "[Counter({'o': 1, 'n': 1}), Counter({'I': 1, 'N': 1})]\n",
      "[Counter({'M': 1, 'o': 1, 'n': 1, 'd': 1, 'a': 1, 'y': 1}), Counter({'N': 2, 'P': 1})]\n",
      "[Counter({'t': 2, 'h': 1, 'a': 1}), Counter({'I': 1, 'N': 1})]\n",
      "[Counter({'WASHINGTON': 1, 'NNP': 1})]\n",
      "[Counter({'-': 2}), Counter({':': 1})]\n",
      "[Counter({'I': 1, 'n': 1}), Counter({'I': 1, 'N': 1})]\n",
      "[Counter({'t': 1, 'h': 1, 'e': 1}), Counter({'D': 1, 'T': 1})]\n",
      "[Counter({'w': 1, 'a': 1, 'k': 1, 'e': 1}), Counter({'N': 2})]\n",
      "[Counter({'o': 1, 'f': 1}), Counter({'I': 1, 'N': 1})]\n",
      "[Counter({'s': 1, 't': 1, 'r': 1, 'i': 1, 'n': 1, 'g': 1}), Counter({'N': 2})]\n",
      "[Counter({'o': 1, 'f': 1}), Counter({'I': 1, 'N': 1})]\n",
      "[Counter({'a': 1, 'b': 1, 'u': 1, 's': 1, 'e': 1}), Counter({'N': 2})]\n",
      "[Counter({'b': 1, 'y': 1}), Counter({'I': 1, 'N': 1})]\n",
      "[Counter({'n': 1, 'e': 1, 'w': 1}), Counter({'J': 2})]\n",
      "[Counter({'y': 1, 'o': 1, 'r': 1, 'k': 1}), Counter({'N': 2})]\n",
      "[Counter({'p': 1, 'o': 1, 'l': 1, 'i': 1, 'c': 1, 'e': 1}), Counter({'N': 2})]\n",
      "[Counter({'f': 2, 'o': 1, 'i': 1, 'c': 1, 'e': 1, 'r': 1, 's': 1}), Counter({'N': 2, 'S': 1})]\n",
      "[Counter({'i': 1, 'n': 1}), Counter({'I': 1, 'N': 1})]\n",
      "[Counter({'t': 1, 'h': 1, 'e': 1}), Counter({'D': 1, 'T': 1})]\n",
      "[Counter({'9': 2, '1': 1, '0': 1, 's': 1}), Counter({'C': 1, 'D': 1})]\n",
      "[Counter({',': 1}), Counter({',': 1})]\n",
      "[Counter({'Loretta': 1, 'NNP': 1}), Counter({'E.Lynch': 1, 'NNP': 1})]\n",
      "[Counter({',': 1}), Counter({',': 1})]\n",
      "[Counter({'t': 1, 'h': 1, 'e': 1}), Counter({'D': 1, 'T': 1})]\n",
      "[Counter({'Top': 1, 'NNP': 1})]\n",
      "[Counter({'e': 2, 'f': 1, 'd': 1, 'r': 1, 'a': 1, 'l': 1}), Counter({'J': 2})]\n",
      "[Counter({'r': 2, 'o': 2, 'p': 1, 's': 1, 'e': 1, 'c': 1, 'u': 1, 't': 1}), Counter({'N': 2})]\n",
      "[Counter({'i': 1, 'n': 1}), Counter({'I': 1, 'N': 1})]\n",
      "[Counter({'o': 2, 'b': 1, 'r': 1, 'k': 1, 'l': 1, 'y': 1, 'n': 1}), Counter({'N': 2})]\n",
      "[Counter({',': 1}), Counter({',': 1})]\n",
      "[Counter({'s': 1, 'p': 1, 'o': 1, 'k': 1, 'e': 1}), Counter({'V': 1, 'B': 1, 'D': 1})]\n",
      "[Counter({'f': 2, 'l': 2, 'o': 1, 'r': 1, 'c': 1, 'e': 1, 'u': 1, 'y': 1}), Counter({'R': 1, 'B': 1})]\n",
      "[Counter({'a': 1, 'b': 1, 'o': 1, 'u': 1, 't': 1}), Counter({'I': 1, 'N': 1})]\n",
      "[Counter({'t': 1, 'h': 1, 'e': 1}), Counter({'D': 1, 'T': 1})]\n",
      "[Counter({'p': 1, 'a': 1, 'i': 1, 'n': 1}), Counter({'N': 2})]\n",
      "[Counter({'o': 1, 'f': 1}), Counter({'I': 1, 'N': 1})]\n",
      "[Counter({'a': 1}), Counter({'D': 1, 'T': 1})]\n",
      "[Counter({'b': 1, 'r': 1, 'o': 1, 'k': 1, 'e': 1, 'n': 1}), Counter({'J': 2})]\n",
      "[Counter({'t': 2, 'r': 1, 'u': 1, 's': 1}), Counter({'N': 2})]\n",
      "[Counter({'t': 2, 'h': 1, 'a': 1}), Counter({'I': 1, 'N': 1})]\n",
      "[Counter({'A': 2, 'r': 2, 'i': 2, 'c': 2, 'a': 2, 'f': 1, '-': 1, 'm': 1, 'e': 1, 'n': 1, 's': 1}), Counter({'N': 2, 'P': 1})]\n",
      "[Counter({'f': 1, 'e': 1, 'l': 1, 't': 1}), Counter({'V': 1, 'B': 1, 'D': 1})]\n",
      "[Counter({'a': 1, 'n': 1, 'd': 1}), Counter({'C': 2})]\n",
      "[Counter({'s': 1, 'a': 1, 'i': 1, 'd': 1}), Counter({'V': 1, 'B': 1, 'D': 1})]\n",
      "[Counter({'t': 1, 'h': 1, 'e': 1}), Counter({'D': 1, 'T': 1})]\n",
      "[Counter({'i': 3, 's': 2, 'r': 1, 'e': 1, 'p': 1, 'o': 1, 'n': 1, 'b': 1, 'l': 1, 't': 1, 'y': 1}), Counter({'N': 2})]\n",
      "[Counter({'f': 1, 'o': 1, 'r': 1}), Counter({'I': 1, 'N': 1})]\n",
      "[Counter({'r': 2, 'i': 2, 'e': 1, 'p': 1, 'a': 1, 'n': 1, 'g': 1}), Counter({'V': 1, 'B': 1, 'G': 1})]\n",
      "[Counter({'e': 2, 'n': 2, 'g': 1, 'r': 1, 'a': 1, 't': 1, 'i': 1, 'o': 1, 's': 1}), Counter({'N': 2, 'S': 1})]\n",
      "[Counter({'o': 1, 'f': 1}), Counter({'I': 1, 'N': 1})]\n",
      "[Counter({'c': 2, 'o': 2, 'm': 2, 'n': 2, 'i': 2, 'u': 1, 'a': 1, 't': 1}), Counter({'N': 2})]\n",
      "[Counter({'a': 1, 'n': 1, 'd': 1}), Counter({'C': 2})]\n",
      "[Counter({'s': 2, 't': 2, 'm': 1, 'i': 1, 'r': 1, 'u': 1}), Counter({'N': 2})]\n",
      "[Counter({'l': 2, 'f': 1, 'e': 1}), Counter({'V': 1, 'B': 1, 'D': 1})]\n",
      "[Counter({'t': 1, 'o': 1}), Counter({'T': 1, 'O': 1})]\n",
      "[Counter({'l': 1, 'a': 1, 'w': 1}), Counter({'N': 2})]\n",
      "[Counter({'e': 3, 'n': 2, 'f': 1, 'o': 1, 'r': 1, 'c': 1, 'm': 1, 't': 1}), Counter({'N': 2})]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence1))):\n",
    "    if                                  (chunk, 'label'):\n",
    "        print([Counter(label) for label in chunk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rajkumar', 'WASHINGTON', 'police officers', 'Loretta E.Lynch', 'Top']\n"
     ]
    }
   ],
   "source": [
    "#a)\n",
    "sentence1=\"Rajkumar said on Monday that WASHINGTON--In the wake of string of abuse by new york police officers in the 1990s,Loretta E.Lynch,the Top federal prosecutor in brooklyn ,spoke forcefully about the pain of a broken trust that Africa-Americans felt and said the responsibility for repairing generations of communication and mistrust fell to law enforcement\"\n",
    "word = nltk.word_tokenize(sentence1)\n",
    "pos_tag = nltk.pos_tag(word)\n",
    "chunk = nltk.ne_chunk(pos_tag)\n",
    "grammar = \"NP: {<NN><NNS>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(chunk)\n",
    "NE = [ \" \".join(w for w, t in ele) for ele in result if isinstance(ele, nltk.Tree)]\n",
    "print (NE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WRITE A REGULAR EXPRESSION PATTER TO DETECT THIS.YOU WILL NEED NLTK.REGEXPPARSER CLASS TO DEFINE PATTERN AND PARSE TERMS TO DETECT PATTERNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rajkumar', 'WASHINGTON', 'the wake', 'Loretta E.Lynch', 'Top', 'the pain', 'a broken trust', 'the responsibility']\n"
     ]
    }
   ],
   "source": [
    "#b)\n",
    "grammar = \"NP: {<DT><JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(chunk)\n",
    "NE = [ \" \".join(w for w, t in ele) for ele in result if isinstance(ele, nltk.Tree)]\n",
    "print (NE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Rajkumar', 'NNP'), ('said', 'VBD'), ('on', 'IN'), ('Monday', 'NNP'), ('that', 'IN'), ('WASHINGTON', 'NNP'), ('--', ':'), ('In', 'IN'), Tree('NP', [('the', 'DT'), ('wake', 'NN')]), ('of', 'IN'), ('string', 'NN'), ('of', 'IN'), ('abuse', 'NN'), ('by', 'IN'), ('new', 'JJ'), ('york', 'NN'), ('police', 'NN'), ('officers', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('1990s', 'CD'), (',', ','), ('Loretta', 'NNP'), ('E.Lynch', 'NNP'), (',', ','), ('the', 'DT'), ('Top', 'NNP'), ('federal', 'JJ'), ('prosecutor', 'NN'), ('in', 'IN'), ('brooklyn', 'NN'), (',', ','), ('spoke', 'VBD'), ('forcefully', 'RB'), ('about', 'IN'), Tree('NP', [('the', 'DT'), ('pain', 'NN')]), ('of', 'IN'), Tree('NP', [('a', 'DT'), ('broken', 'JJ'), ('trust', 'NN')]), ('that', 'IN'), ('Africa-Americans', 'NNP'), ('felt', 'VBD'), ('and', 'CC'), ('said', 'VBD'), Tree('NP', [('the', 'DT'), ('responsibility', 'NN')]), ('for', 'IN'), ('repairing', 'VBG'), ('generations', 'NNS'), ('of', 'IN'), ('communication', 'NN'), ('and', 'CC'), ('mistrust', 'NN'), ('fell', 'VBD'), ('to', 'TO'), ('law', 'NN'), ('enforcement', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "parse = cp.parse(tags)\n",
    "print(parse[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a regular expression pattern to detect this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rajkumar', 'WASHINGTON', 'the wake', 'Loretta E.Lynch', 'Top', 'the pain', 'the responsibility']\n"
     ]
    }
   ],
   "source": [
    "grammar = \"NP: {<DT><JACJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(chunk)\n",
    "NE = [ \" \".join(w for w, t in ele) for ele in result if isinstance(ele, nltk.Tree)]\n",
    "print (NE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract All Named Entities From The Following Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2=\"European authrities fined google a record $5.1 billion on Wednesday for abusing Its Power in the Mobile phone market and ordered the company to alter its practices\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('GPE', [('European', 'JJ')]), ('authrities', 'NNS'), ('fined', 'VBN'), ('google', 'VBP'), ('a', 'DT'), ('record', 'NN'), ('$', '$'), ('5.1', 'CD'), ('billion', 'CD'), ('on', 'IN'), ('Wednesday', 'NNP'), ('for', 'IN'), ('abusing', 'VBG'), ('Its', 'PRP$'), ('Power', 'NN'), ('in', 'IN'), ('the', 'DT'), ('Mobile', 'NNP'), ('phone', 'NN'), ('market', 'NN'), ('and', 'CC'), ('ordered', 'VBD'), ('the', 'DT'), ('company', 'NN'), ('to', 'TO'), ('alter', 'VB'), ('its', 'PRP$'), ('practices', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "token=word_tokenize(sentence2)\n",
    "tag=nltk.pos_tag(token)\n",
    "ne_tree=ne_chunk(tag)\n",
    "print(ne_tree[:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['European', '5.1', 'billion']\n"
     ]
    }
   ],
   "source": [
    "word = nltk.word_tokenize(sentence2) \n",
    "pos_tag = nltk.pos_tag(word) \n",
    "chunk = nltk.ne_chunk(pos_tag) \n",
    "grammar = \"NP: {<CD>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(chunk)\n",
    "NE = [ \" \".join(w for w, t in ele) for ele in result if isinstance(ele, nltk.Tree)] \n",
    "print (NE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['European', 'a record', 'the company']\n"
     ]
    }
   ],
   "source": [
    "word = nltk.word_tokenize(sentence2) \n",
    "pos_tag = nltk.pos_tag(word) \n",
    "chunk = nltk.ne_chunk(pos_tag) \n",
    "grammar = \"NP: {<DT><JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(chunk)\n",
    "NE = [ \" \".join(w for w, t in ele) for ele in result if isinstance(ele, nltk.Tree)] \n",
    "print (NE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = \"Serves: 4\\n\\n1 1/2 cups dry red wine\\n3 cloves garlic\\n1 3/4 cups beef broth\\n1 1/4 cups chicken broth\\n1 1/2 tablespoons tomato paste\\n1 bay leaf\\n1 sprig thyme\\n8 ounces bacon cut into 1/4 inch pieces\\n1 tablespoon flour\\n1 tablespoon butter\\n4 1 inch rib-eye steaks\\n1 tablespoon bourbon whiskey\"\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "words = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "named_entities = []\n",
    "for sentence in words:\n",
    "    tagged = nltk.pos_tag(sentence)\n",
    "    entities = nltk.ne_chunk(tagged, binary=False)\n",
    "    for entity in entities:\n",
    "        if hasattr(entity, 'label') and entity.label() == 'NE':\n",
    "            named_entities.append(entity[0][0])\n",
    "print(named_entities)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dry red wine',\n",
       " 'garlic',\n",
       " 'beef broth',\n",
       " 'chicken broth',\n",
       " 'tomato paste',\n",
       " 'bay leaf',\n",
       " 'thyme',\n",
       " 'bacon',\n",
       " 'flour',\n",
       " 'butter',\n",
       " 'rib-eye steaks',\n",
       " 'bourbon whiskey']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['dry red wine', 'garlic', 'beef broth', 'chicken broth', 'tomato paste', 'bay leaf', 'thyme', 'bacon', 'flour', 'butter', 'rib-eye steaks', 'bourbon whiskey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
