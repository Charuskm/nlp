{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "383a4661",
   "metadata": {},
   "source": [
    "Roll:no:225229106"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8dee0b",
   "metadata": {},
   "source": [
    "# Lab-5 : Stemming and Lemmatization on Movie Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acb4df5",
   "metadata": {},
   "source": [
    "# EXCERCISE:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0d46434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import glob\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "967569dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name                                             Modified             Size\n",
      "movies/                                        2018-01-19 08:32:38            0\n",
      "movies/12 Angry Men.txt                        2018-01-17 20:40:42         1007\n",
      "movies/12 Years a Slave.txt                    2018-01-17 20:42:50         6451\n",
      "movies/4 Months, 3 Weeks and 2 Days.txt        2018-01-17 20:37:10         1151\n",
      "movies/All About Eve.txt                       2018-01-17 20:33:18         1346\n",
      "movies/American Graffiti.txt                   2018-01-17 20:44:30         3417\n",
      "movies/Boyhood.txt                             2018-01-17 20:27:14         1970\n",
      "movies/Casablanca.txt                          2018-01-17 20:26:26         1896\n",
      "movies/Citizen Kane.txt                        2018-01-17 20:23:56         1483\n",
      "movies/Gone with the Wind.txt                  2018-01-17 20:38:10         1318\n",
      "movies/Hoop Dreams.txt                         2018-01-17 20:34:12         7909\n",
      "movies/Manchester by the Sea.txt               2018-01-17 20:40:06         3674\n",
      "movies/Moonlight.txt                           2018-01-17 20:31:42         2323\n",
      "movies/My Left Foot.txt                        2018-01-17 20:38:50         1115\n",
      "movies/Pan's Labyrinth.txt                     2018-01-17 20:32:18         4431\n",
      "movies/Psycho.txt                              2018-01-17 20:34:46         3727\n",
      "movies/Ran.txt                                 2018-01-17 20:43:48         2207\n",
      "movies/Singin' in the Rain.txt                 2018-01-17 20:29:42          782\n",
      "movies/Some Like It Hot.txt                    2018-01-17 20:35:40         7489\n",
      "movies/The Godfather.txt                       2018-01-17 20:25:32         4293\n",
      "movies/Three Colors Red.txt                    2018-01-17 20:28:22         2892\n"
     ]
    }
   ],
   "source": [
    "file_name = \"movies.zip\"\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "    zip.printdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07cefc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d076a5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.stem import LancasterStemmer\n",
    "ls = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06209dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [file for file in glob.glob(\"movies/*\")]\n",
    "for f in files:\n",
    "    fi=open(f, 'r', encoding='utf-8')\n",
    "    contents = fi.readlines()\n",
    "    print(contents)\n",
    "        #print(\"**********************************************************\")\n",
    "        #print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd91691",
   "metadata": {},
   "source": [
    "### A. How many sentences in each file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4330c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [file for file in glob.glob(\"movies/*\")]\n",
    "for file in files:\n",
    "    with open(file, 'r', encoding='cp1252') as f:\n",
    "        contents = f.readlines()\n",
    "        for row in contents:\n",
    "            sent_text = nltk.sent_tokenize(row)\n",
    "            print(\"sentence tokenize \", len(sent_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b7773c",
   "metadata": {},
   "source": [
    "### B. How many tokens in each file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c33584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [file for file in glob.glob(\"movies/*\")]\n",
    "for file in files:\n",
    "    with open(file, 'r', encoding='cp1252') as f:\n",
    "        contents = f.readlines()\n",
    "        for row1 in contents:\n",
    "            words = nltk.word_tokenize(row1)\n",
    "        print(\"word tokenize \", len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37ce761",
   "metadata": {},
   "source": [
    "### C. How many tokens excluding stop words in each file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "165eda39",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [file for file in glob.glob(\"movies/*\")]\n",
    "for file in files:\n",
    "    with open(file, 'r', encoding='cp1252') as f:\n",
    "        contents = f.readlines()\n",
    "        filtered_sentence = [w for w in words if not w in stop_words]\n",
    "        print(\"stopwords \", len(filtered_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b43c5",
   "metadata": {},
   "source": [
    "### D. How many unique stems (ie., stemming) in each file? (Use PorterStemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c647d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def port_stemSentence(sentence):\n",
    "    tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    tok = tokenizer.tokenize(sentence)\n",
    "    filtered_sentence = [w for w in tok if not w in stop_words]\n",
    "    stem_sentence = []\n",
    "    for word in filtered_sentence:\n",
    "        stem_sentence.append(ps.stem(word))\n",
    "    return len(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "163670d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [file for file in glob.glob(\"movies/*\")]\n",
    "for file in files:\n",
    "    with open(file, 'r', encoding='cp1252') as f:\n",
    "        contents = f.readline()\n",
    "        print(\"porter_stemming \")\n",
    "        print(port_stemSentence(contents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735fc4d1",
   "metadata": {},
   "source": [
    "### E. How many unique stems (ie., stemming) in each file? (Use LancasterStemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18f7180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lan_stemSentence(sentence):\n",
    "    tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    tok = tokenizer.tokenize(sentence)\n",
    "    filtered_sentence = [w for w in tok if not w in stop_words]\n",
    "    stem_sentence = []\n",
    "    for word in filtered_sentence:\n",
    "        stem_sentence.append(ls.stem(word))\n",
    "    return len(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4dc4801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [file for file in glob.glob(\"movies/*\")]\n",
    "for file in files:\n",
    "    with open(file, 'r', encoding='cp1252') as f:\n",
    "        contents = f.readline()\n",
    "        print(\"lancaster_stemming \")\n",
    "        print(port_stemSentence(contents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb63453",
   "metadata": {},
   "source": [
    "### F. How many unique words (ie., lemmatization) in each file? (Use WordNetLemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4885e78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93b2def0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmSentence(sentence):\n",
    "    tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    tok = tokenizer.tokenize(sentence)\n",
    "    filtered_sentence = [w for w in tok if not w in stop_words]\n",
    "    lemm_sentence = []\n",
    "    for word in filtered_sentence:\n",
    "        lemm_sentence.append(lemmatizer.lemmatize(word))\n",
    "    return len(lemm_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5b8212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    with open(file, 'r', encoding='cp1252') as f:\n",
    "        contents = f.readline()\n",
    "        print(\"lemmatization \")\n",
    "        print(lemmSentence(contents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9757fc2f",
   "metadata": {},
   "source": [
    "### EXERCISE-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70be9b62",
   "metadata": {},
   "source": [
    "### Step-1 For each movie:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79ec06a",
   "metadata": {},
   "source": [
    "### Tokenize terms and build list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcf866c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = []\n",
    "for file in files:\n",
    "    with open(file,'r',encoding='cp1252') as f:\n",
    "        contents = f.read()\n",
    "        let=tokenizer.tokenize(contents)\n",
    "        tok.append(let)\n",
    "tok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd59b32",
   "metadata": {},
   "source": [
    "### Find lemmatized words from the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2edba662",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b650817d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_lem =[]\n",
    "for i in tok:\n",
    "    for j in i:\n",
    "        to_lem = lemmatizer.lemmatize(j)\n",
    "        tok_lem.append(to_lem)\n",
    "tok_lem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6247153",
   "metadata": {},
   "source": [
    "### Step-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89e1c71",
   "metadata": {},
   "source": [
    "### Build Term-Document matrix using TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d02c61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    with open(file,'r',encoding='cp1252') as f:\n",
    "        contents = f.read()\n",
    "        tok = tokenizer.tokenize(contents)\n",
    "        filtered_sentence = [w for w in tok if not w in stop_words]\n",
    "        tfidf = TfidfVectorizer(min_df=2,max_df=0.5,ngram_range=(1,2))\n",
    "        features = tfidf.fit_transform(filtered_sentence)\n",
    "        df = pd.DataFrame(features.todense(),columns=tfidf.get_feature_names())\n",
    "        print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fe5cc1",
   "metadata": {},
   "source": [
    "### Step-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443a8f1f",
   "metadata": {},
   "source": [
    "### Take vectors of any two movies and compute cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c2efa99",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[43mfiles\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m,encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcp1252\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      2\u001b[0m     contents \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      3\u001b[0m     tok \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(contents)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "with open(files[5],'r',encoding='cp1252')as f:\n",
    "    contents = f.read()\n",
    "    tok = tokenizer.tokenize(contents)\n",
    "    filtered_sentence = [w for w in tok if not w in stop_words]\n",
    "    tfidf = TfidfVectorizer(min_df=2,max_df=0.5,ngram_range=(1,2))\n",
    "    movie1 = tfidf.fit_transform(filtered_sentence)\n",
    "    print(movie1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e37f673c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[43mfiles\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m,encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcp1252\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      2\u001b[0m     contents \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      3\u001b[0m     tok \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(contents)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "with open(files[10],'r',encoding='cp1252')as f:\n",
    "    contents = f.read()\n",
    "    tok = tokenizer.tokenize(contents)\n",
    "    filtered_sentence = [w for w in tok if not w in stop_words]\n",
    "    tfidf = TfidfVectorizer(min_df=2,max_df=0.5,ngram_range=(1,2))\n",
    "    movie2 = tfidf.fit_transform(filtered_sentence)\n",
    "    print(movie2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9d931a",
   "metadata": {},
   "source": [
    "# EXERCISE-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43040a33",
   "metadata": {},
   "source": [
    "# will lemmatized matrix help to achieve better similarity search or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf812b0",
   "metadata": {},
   "source": [
    "\n",
    "Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meanings to one word.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
